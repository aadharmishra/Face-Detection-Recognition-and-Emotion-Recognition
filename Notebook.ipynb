{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the required libraries\n",
    "import cv2\n",
    "import os\n",
    "import sys\n",
    "import face_recognition\n",
    "import numpy as np\n",
    "from PyEmotion import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***FACE DETECTION, FACE RECOGNITION & EMOTION RECOGNITION***\n",
      "\n",
      "***MAIN MENU***\n",
      "\n",
      "1.Face Detection & Feature Recognition\n",
      "2.Face Recognition\n",
      "3.Emotion Recognition\n",
      "4.Exit\n",
      "\n",
      "Enter your choice: 3\n",
      "Welcome to Emotion Recognition!\n",
      "\n",
      "\u001b[35m ____          _____                    _    _               \n",
      "|  _ \\  _   _ | ____| _ __ ___    ___  | |_ (_)  ___   _ __  \n",
      "| |_) || | | ||  _|  | '_ ` _ \\  / _ \\ | __|| | / _ \\ | '_ \\ \n",
      "|  __/ | |_| || |___ | | | | | || (_) || |_ | || (_) || | | |\n",
      "|_|     \\__, ||_____||_| |_| |_| \\___/  \\__||_| \\___/ |_| |_|\n",
      "        |___/                                                \n",
      "\u001b[0m\n",
      "\u001b[35mWelcome to PyEmotion \u001b[0m\n",
      "Please wait while the camera is being configured.\n",
      "\n",
      "Thank you for using emotion recognition!\n",
      "\n",
      "***END OF PROGRAM***\n"
     ]
    }
   ],
   "source": [
    "#Python program to detect face, its features,recognize it and detect facial emotion in real-time using webcam.\n",
    "\n",
    "print(\"***FACE DETECTION, FACE RECOGNITION & EMOTION RECOGNITION***\")\n",
    "print()\n",
    "print(\"***MAIN MENU***\")\n",
    "print()\n",
    "print(\"1.Face Detection & Feature Recognition\")\n",
    "print(\"2.Face Recognition\")\n",
    "print(\"3.Emotion Recognition\")\n",
    "print(\"4.Exit\")\n",
    "print()\n",
    "\n",
    "\n",
    "c = int(input(\"Enter your choice: \"))\n",
    "\n",
    "while(1):\n",
    "    if c==1:\n",
    "        print(\"Welcome to face detection & feature recognition!\")\n",
    "        print()\n",
    "        print(\"Please wait while the camera is being configured.\")\n",
    "        print()\n",
    "        \n",
    "        #Method to draw boundary around detected features \n",
    "        def draw_boundary(img, classifier, scaleFactor, minNeighbors, color, text):\n",
    "            \n",
    "            #For detection, conversion grayscale is needed \n",
    "            gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "            \n",
    "            #Generic function which detects features once classifier is passed.\n",
    "            #By Scaling factor we can scale our classifier to detect a small or large face \n",
    "            #depending upon the face's proximity to the camera.\n",
    "            #Minimum neigbours basically means how many minimum features are to be detected so that the thing under \n",
    "            #observation can be classified as a face or not.\n",
    "            #It returns a list of features.\n",
    "            features = classifier.detectMultiScale(gray_img, scaleFactor, minNeighbors)\n",
    "            \n",
    "            #List which holds coordinates for face\n",
    "            coords = []\n",
    "            for (x, y, w, h) in features:\n",
    "                \n",
    "                #Drawing a rectangle around detected feature\n",
    "                cv2.rectangle(img, (x,y), (x+w, y+h), color, 2)\n",
    "                \n",
    "                #Labelling the detected feature \n",
    "                cv2.putText(img, text, (x,y-4), cv2.FONT_HERSHEY_SIMPLEX, 0.8, color, 1, cv2.LINE_AA)\n",
    "                \n",
    "                #Update the coordinates\n",
    "                coords = [x, y, w, h]\n",
    "    \n",
    "            #return the updated image(frame)\n",
    "            return coords\n",
    "        \n",
    "        #Function which takes the frame and classifiers and returns an updated frame \n",
    "        #which shows all the detected features along with the face\n",
    "        def detect(img, faceCascade, eyesCascade, noseCascade, mouthCascade):\n",
    "            \n",
    "            #creating a dictionary for colours so as to easily access them when required\n",
    "            color = {\"blue\":(255,0,0), \"red\":(0,0,255), \"green\":(0,255,0), \"white\":(255,255,255)}\n",
    "            \n",
    "            coords = draw_boundary(img, faceCascade, 1.1, 10, color['blue'], \"Face\")\n",
    "            \n",
    "            #To check if face has been detected or not, and if it has been detected,\n",
    "            #then crop the frame to our region of interest, update it, and then\n",
    "            #use this updated frame to draw rectangles around the detected features\n",
    "            #by passing respective classifiers and other parameters using draw_boundary function\n",
    "            if len(coords) == 4:\n",
    "                #updated the coordinates, cropped the video frame to region of interest\n",
    "                roi_img = img[coords[1]:coords[1]+coords[3], coords[0]:coords[0]+coords[2]]\n",
    "                \n",
    "                #Detecting eyes,nose and mouth in our region of interest\n",
    "                coords = draw_boundary(roi_img, eyesCascade, 1.1, 14, color['red'], \"Eyes\")\n",
    "                coords = draw_boundary(roi_img, noseCascade, 1.1, 5, color['green'], \"Nose\")\n",
    "                coords = draw_boundary(roi_img, mouthCascade, 1.1, 20, color['white'], \"Mouth\")\n",
    "    \n",
    "            #returning the updated frame\n",
    "            return img\n",
    "        \n",
    "        #Loading and passing the classifiers for face and feature detection\n",
    "        faceCascade = cv2.CascadeClassifier(\"haarcascade_frontalface_default.xml\")\n",
    "        eyesCascade = cv2.CascadeClassifier(\"haarcascade_eye.xml\")\n",
    "        noseCascade = cv2.CascadeClassifier(\"Nariz.xml\")\n",
    "        mouthCascade = cv2.CascadeClassifier(\"Mouth.xml\")\n",
    "        \n",
    "        # Get a reference to webcam 0 (the default one)  \n",
    "        video_capture = cv2.VideoCapture(0)\n",
    "        \n",
    "        #Infinite loop to detect face and its features \n",
    "        while True:\n",
    "            #Captures a frame from the video\n",
    "            _, img = video_capture.read()\n",
    "            \n",
    "            #Detects face and its features in that frame\n",
    "            img = detect(img, faceCascade, eyesCascade, noseCascade, mouthCascade)\n",
    "            \n",
    "            #Shows the updated frame \n",
    "            cv2.imshow(\"Face Detection\", img)\n",
    "            \n",
    "            #Press the button 'q' to exit the video\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "        \n",
    "        #Releases the webcam & destroys all the windows\n",
    "        video_capture.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        print(\"Thank you for using face detection!\")\n",
    "        print()\n",
    "        break\n",
    "        \n",
    "    elif c==2:\n",
    "        print(\"Welcome to face recognition\")\n",
    "        print()\n",
    "        print(\"Please wait while the camera is being configured.\")\n",
    "        print()\n",
    "        \n",
    "        # Get a reference to webcam 0 (the default one)\n",
    "        video_capture = cv2.VideoCapture(0)\n",
    "\n",
    "        # Loading a sample picture and learning how to recognize it.\n",
    "        modi_image = face_recognition.load_image_file(\"modi.jfif\")\n",
    "        modi_face_encoding = face_recognition.face_encodings(modi_image)[0]\n",
    "\n",
    "        # Loading a second sample picture and learning how to recognize it.\n",
    "        zuckerberg_image = face_recognition.load_image_file(\"zuckerberg.jfif\")\n",
    "        zuckerberg_face_encoding = face_recognition.face_encodings(zuckerberg_image)[0]\n",
    "\n",
    "\n",
    "        # Create arrays of known face encodings and their names\n",
    "        known_face_encodings = [\n",
    "            modi_face_encoding,\n",
    "            zuckerberg_face_encoding,\n",
    "        ]\n",
    "        known_face_names = [\n",
    "            \"Narendra Modi\",\n",
    "            \"Mark Zuckerberg\"\n",
    "        ]\n",
    "\n",
    "        # Creating lists \n",
    "        face_locations = []\n",
    "        face_encodings = []\n",
    "        face_names = []\n",
    "        process_this_frame = True\n",
    "        \n",
    "        while True:\n",
    "            # Grabbing a single frame of video\n",
    "            ret, frame = video_capture.read()\n",
    "\n",
    "            # Resizing frame of video to 1/4 size for faster face recognition processing\n",
    "            small_frame = cv2.resize(frame, (0, 0), fx=0.25, fy=0.25)\n",
    "\n",
    "            # Converting the image from BGR color (which OpenCV uses) to RGB color (which face_recognition uses)\n",
    "            rgb_small_frame = small_frame[:, :, ::-1]\n",
    "\n",
    "            # Only processing every other frame of video to save time\n",
    "            if process_this_frame:\n",
    "                # Finding all the faces and face encodings in the current frame of video\n",
    "                face_locations = face_recognition.face_locations(rgb_small_frame)\n",
    "                face_encodings = face_recognition.face_encodings(rgb_small_frame, face_locations)\n",
    "\n",
    "                face_names = []\n",
    "                for face_encoding in face_encodings:\n",
    "                    # Seeing if the face is a match for the known face(s) by comparing the list\n",
    "                    # of face encodings against a candidate encoding to see if they match, returns\n",
    "                    # a list of True/false values\n",
    "                    matches = face_recognition.compare_faces(known_face_encodings, face_encoding)\n",
    "                    name = \"Unknown\"\n",
    "\n",
    "                    #If a match was found in known_face_encodings, just using the first one.\n",
    "                    # if True in matches:\n",
    "                    #     first_match_index = matches.index(True)\n",
    "                    #     name = known_face_names[first_match_index]\n",
    "\n",
    "                    # Or instead, using the known face with the smallest(min) distance(euclidian) to the new face.\n",
    "                    # Given a list of face encodings, compare them to a known face encoding & get a\n",
    "                    # euclidean distance for each comparision face. This tells us how similar the faces are.\n",
    "                    face_distances = face_recognition.face_distance(known_face_encodings, face_encoding)\n",
    "                    best_match_index = np.argmin(face_distances)\n",
    "                    if matches[best_match_index]:\n",
    "                        name = known_face_names[best_match_index]\n",
    "\n",
    "                    face_names.append(name)\n",
    "\n",
    "            process_this_frame = not process_this_frame\n",
    "            \n",
    "            # Display the results\n",
    "            for (top, right, bottom, left), name in zip(face_locations, face_names):\n",
    "                # Scale back up face locations since the frame we detected in was scaled to 1/4 its size\n",
    "                top *= 4\n",
    "                right *= 4\n",
    "                bottom *= 4\n",
    "                left *= 4\n",
    "\n",
    "                # Draw a box around the face\n",
    "                cv2.rectangle(frame, (left, top), (right, bottom), (0, 0, 255), 2)\n",
    "\n",
    "                # Draw a label with a name below the face\n",
    "                cv2.rectangle(frame, (left, bottom - 25), (right, bottom), (0, 0, 255), cv2.FILLED)\n",
    "                font = cv2.FONT_HERSHEY_DUPLEX\n",
    "                cv2.putText(frame, name, (left + 6, bottom - 6), font, 0.6, (255, 255, 255), 1, cv2.LINE_AA)\n",
    "\n",
    "            # Display the resulting image\n",
    "            cv2.imshow('Face Recognition', frame)\n",
    "\n",
    "            # To quit, hit 'q' on the keyboard \n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "            \n",
    "        # Release handle to the webcam\n",
    "        video_capture.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        break\n",
    "        \n",
    "    elif c==3:\n",
    "        print(\"Welcome to Emotion Recognition!\")\n",
    "        print()\n",
    "\n",
    "        #inbuilt function inside the library\n",
    "        PyEmotion()\n",
    "        \n",
    "        print(\"Please wait while the camera is being configured.\")\n",
    "        print()\n",
    "        \n",
    "        #To detect face(init PyEmotion function)\n",
    "        #The __init__ method can be called when an object is created from the class, \n",
    "        #and access is required to initialize the attributes of the class.\n",
    "        er = DetectFace(device='cpu', gpu_id=0)\n",
    "\n",
    "        # Opening default camera\n",
    "        cap = cv2.VideoCapture(0)\n",
    "        \n",
    "        #This code initiates an infinite loop (to be broken later by a break statement), \n",
    "        #where we have ret and frame being defined as the cap.read(). \n",
    "        #Basically, ret is a boolean regarding whether or not there was a return at all, \n",
    "        #at the frame is each frame that is returned. \n",
    "        #If there is no frame, we won't get an error, we will get None.\n",
    "        while(True):\n",
    "            \n",
    "            #Reads a single frame from the video\n",
    "            ret, frame = cap.read()\n",
    "            \n",
    "            #Predicts emotion in that frame where \n",
    "            #frame->returns the frame with emotion & \n",
    "            #emotion->returns the frame of the emotion\n",
    "            frame, emotion = er.predict_emotion(frame)\n",
    "            \n",
    "            #Displays the predicted emotion\n",
    "            cv2.imshow('Emotion detection', frame)\n",
    "            \n",
    "            #To quit, enter the 'q' button in keyboard\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "        \n",
    "        #Releases the webcam & destroys all the windows\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        print(\"Thank you for using emotion recognition!\")\n",
    "        print()\n",
    "        break\n",
    "        \n",
    "    else:\n",
    "        print(\"Thank you!\")\n",
    "        print()\n",
    "        break       \n",
    "        \n",
    "print(\"***END OF PROGRAM***\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
